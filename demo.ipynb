{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Quantum Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a QCNN trained to classify the different particle types in LAr detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pennylane in /Users/1zd/Library/Python/3.9/lib/python/site-packages (0.32.0)\n",
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.36.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 526 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.26.4)\n",
      "Requirement already satisfied: appdirs in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: networkx in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (3.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (4.7.1)\n",
      "Requirement already satisfied: requests in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (2.31.0)\n",
      "Requirement already satisfied: cachetools in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (5.3.1)\n",
      "Requirement already satisfied: toml in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (0.6.6)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: rustworkx in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (0.13.1)\n",
      "Collecting pennylane-lightning>=0.36\n",
      "  Downloading PennyLane_Lightning-0.36.0-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.11.2)\n",
      "Requirement already satisfied: autograd in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.5)\n",
      "Requirement already satisfied: future>=0.15.2 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/1zd/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (3.4)\n",
      "Installing collected packages: pennylane-lightning, pennylane\n",
      "  Attempting uninstall: pennylane-lightning\n",
      "    Found existing installation: PennyLane-Lightning 0.32.0\n",
      "    Uninstalling PennyLane-Lightning-0.32.0:\n",
      "      Successfully uninstalled PennyLane-Lightning-0.32.0\n",
      "  Attempting uninstall: pennylane\n",
      "    Found existing installation: PennyLane 0.32.0\n",
      "    Uninstalling PennyLane-0.32.0:\n",
      "      Successfully uninstalled PennyLane-0.32.0\n",
      "Successfully installed pennylane-0.36.0 pennylane-lightning-0.36.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pennylane --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zd/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inspect\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates.embeddings import AmplitudeEmbedding, AngleEmbedding\n",
    "from pennylane import numpy as np\n",
    "import autograd.numpy as anp\n",
    "\n",
    "n_qubit = 8 #number of qubit in the circuit\n",
    "encoding = 'amplitude' #choose the quantum encoding: 'amplitude' or 'angle'\n",
    "num_classes = 10 # choose how many classes: 4, 6, 8, 10\n",
    "all_samples = True #True if you want all the samples, False, if you want only 250 samples for each class\n",
    "seed = 43 #set to None to generate the seed randomly\n",
    "U_params = 15 #number of parameters of F_2 circuit\n",
    "num_layer = 1 #number of convolutional layer repetitions\n",
    "load_params = False #if True load parameters from a file\n",
    "opt = 'Adam' #choose the optimizer: Adam, QNGO, or GDO\n",
    "lr = 0.01 #learning rate\n",
    "epochs = 2 #number of epochs\n",
    "batch_size = 64 #size of batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset: split it in train and test, take only 250 samples if all_samples == False\n",
    "\n",
    "Take only 256 features if the amplitude encoding is applied, otherwise only 8 if the angle encoding is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of subset training data: (2500, 28, 28, 1)\n",
      "Shape of subset training labels: (2500,)\n",
      "Shape of subset training data: (2500, 28, 28, 1)\n",
      "Shape of subset training labels: (2500,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "It loads the MNIST dataset and then it processes the dataset based on the encoding method, number of classes and if we want all the samples. \n",
    "param encoding: indicate the quantum encoding used: 'amplitude' or 'angle'\n",
    "param num_classes: number of classes to be predicted, which samples take from the dataset\n",
    "param all_samples: True if we want all the samples, False to take only 250 samples for each class\n",
    "param seed: random_state seed\n",
    "return X_train, X_test, Y_train, Y_test: the dataset divided in training and test set\n",
    "\"\"\"\n",
    "def data_load_and_process(encoding, num_classes, all_samples, seed):\n",
    "\tif seed != None:\n",
    "\t\ttf.random.set_seed(seed)\n",
    "\t\tnp.random.seed(seed)\n",
    "\n",
    "\t(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "\tx_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\t # normalize the data\n",
    "\n",
    "\t#check is the user want all the samples\n",
    "\tif not all_samples:\n",
    "\t\tnum_examples_per_class = 250\n",
    "\t\tselected_indices = []\n",
    "\n",
    "\t\t# Iterate through each class to select 1000 examples\n",
    "\t\tfor class_label in range(10): \n",
    "\t\t\tindices = np.where(y_train == class_label)[0][:num_examples_per_class]\n",
    "\t\t\tselected_indices.extend(indices)\n",
    "\n",
    "\t\t# Filter the training data to contain only the selected examples\n",
    "\t\tx_train_subset = x_train[selected_indices]\n",
    "\t\ty_train_subset = y_train[selected_indices]\n",
    "\n",
    "\t\t# Shuffle the data\n",
    "\t\tshuffle_indices = np.random.permutation(len(x_train_subset))\n",
    "\t\tx_train = x_train_subset[shuffle_indices]\n",
    "\t\ty_train = y_train_subset[shuffle_indices]\n",
    "\n",
    "\tprint(\"Shape of subset training data:\", x_train.shape)\n",
    "\tprint(\"Shape of subset training labels:\", y_train.shape)\n",
    "\n",
    "\t#take only the number of classes selected\n",
    "\tmask_train = np.isin(y_train, range(0, num_classes))\n",
    "\tmask_test = np.isin(y_test, range(0, num_classes))\n",
    "\n",
    "\tX_train = x_train[mask_train]\n",
    "\tX_test = x_test[mask_test]\t\t\n",
    "\tY_train = y_train[mask_train]\n",
    "\tY_test = y_test[mask_test]\n",
    "\n",
    "\tprint(\"Shape of subset training data:\", X_train.shape)\n",
    "\tprint(\"Shape of subset training labels:\", Y_train.shape)\n",
    "\n",
    "\t#check which encoding is used\n",
    "\t#if amplitude encoding is used, then the 256 most important features are taken using PCA\n",
    "\tif encoding == 'amplitude':\n",
    "\t\tX_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\t\tX_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\t\tpca = PCA(n_components = 256)\n",
    "\t\tX_train = pca.fit_transform(X_train_flat)\n",
    "\t\tX_test = pca.transform(X_test_flat)\n",
    "\t\treturn X_train, X_test, Y_train, Y_test\n",
    "\t#if amplitude encoding is used, then the 8 most important features are taken using PCA\n",
    "\telif encoding == 'angle':\n",
    "\t\tX_train = tf.image.resize(X_train[:], (784, 1)).numpy()\n",
    "\t\tX_test = tf.image.resize(X_test[:], (784, 1)).numpy()\n",
    "\t\tX_train, X_test = tf.squeeze(X_train), tf.squeeze(X_test)\n",
    "\n",
    "\t\tpca = PCA(8)\n",
    "\t\t\n",
    "\t\tX_train = pca.fit_transform(X_train)\n",
    "\t\tX_test = pca.transform(X_test)\n",
    "\n",
    "\t\t# Rescale for angle embedding\n",
    "\t\t\n",
    "\t\tX_train, X_test = (X_train - X_train.min()) * (np.pi / (X_train.max() - X_train.min())),\\\n",
    "\t\t\t\t\t\t  (X_test - X_test.min()) * (np.pi / (X_test.max() - X_test.min()))\n",
    "\t\treturn X_train, X_test, Y_train, Y_test\n",
    "\t\n",
    "X_train, X_test, Y_train, Y_test = data_load_and_process(encoding, num_classes, False, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QCN:\n",
    "1. Create the two circuits which implement the convolutional layer and the circuit which implements the pooling operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "F_1 circuit of the paper\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def CC14(params, wires):\n",
    "\t#U_CC14 r = 1\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.RY(params[i], wires=wires[i])\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.CRX(params[i + len(wires)], wires=[wires[(i - 1) % len(wires)], wires[i]])\n",
    "\t\t\n",
    "\t\n",
    "\t\n",
    "\t#U_CC14 r = -1 or 3\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.RY(params[i + 2 * len(wires)], wires=wires[i])\n",
    "\t\t\n",
    "\tif len(wires) % 3 == 0 or len(wires) == 2:\n",
    "\t\tfor i in range(len(wires) - 1, -1, -1):\n",
    "\t\t\tqml.CRX(params[i + 3 * len(wires)], wires=[wires[i], wires[(i-1) % len(wires)]])\n",
    "\t\t\t\n",
    "\telse:\n",
    "\t\tcontrol = len(wires) - 1\n",
    "\t\ttarget = (control + 3) % len(wires)\n",
    "\t\tfor i in range(len(wires) - 1, -1, -1):\n",
    "\t\t\tqml.CRX(params[i + 3 * len(wires)], wires=[wires[control], wires[target]])\n",
    "\t\t\t\n",
    "\t\t\tcontrol = target\n",
    "\t\t\ttarget = (control + 3) % len(wires)\n",
    "\n",
    "\"\"\"\n",
    "F_2 circuit of the paper\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def U_SU4(params, wires): # 15 params\n",
    "\tqml.U3(params[0], params[1], params[2], wires=wires[0])\n",
    "\tqml.U3(params[3], params[4], params[5], wires=wires[1])\n",
    "\tqml.CNOT(wires=[wires[0], wires[1]])\n",
    "\tqml.RY(params[6], wires=wires[0])\n",
    "\tqml.RZ(params[7], wires=wires[1])\n",
    "\tqml.CNOT(wires=[wires[1], wires[0]])\n",
    "\tqml.RY(params[8], wires=wires[0])\n",
    "\tqml.CNOT(wires=[wires[0], wires[1]])\n",
    "\tqml.U3(params[9], params[10], params[11], wires=wires[0])\n",
    "\tqml.U3(params[12], params[13], params[14], wires=wires[1])\n",
    "\n",
    "\"\"\"\n",
    "It implements the pooling circuit\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def Pooling_ansatz(params, wires): #2 params\n",
    "\tqml.CRZ(params[0], wires=[wires[0], wires[1]])\n",
    "\tqml.PauliX(wires=wires[0])\n",
    "\tqml.CRX(params[1], wires=[wires[0], wires[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the structure of the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quantum Circuits for Convolutional layers\n",
    "param U: unitary that implements the convolution\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "param qubits: array that indicate to which qubit apply the convolutional layer\n",
    "\"\"\"\n",
    "def conv_layer(U, params, U_params, num_layer, qubits):\n",
    "\t\tparam0 = 0\n",
    "\t\tparam1 = len(qubits) * 2\n",
    "\t\t\n",
    "\t\t#add f_1 circuit\n",
    "\t\tfor l in range(num_layer):\n",
    "\t\t\tif len(qubits) == 8: #if it is the first layer, the F_1 circuit is \"divided\"\n",
    "\t\t\t\tfor i in range(0, len(qubits), len(qubits)//2):\n",
    "\t\t\t\t\tU(params[param0: param1], wires = qubits[i: i + len(qubits)//2])\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam1 += len(qubits) * 2\n",
    "\t\t\t\tU(params[param0: param1], wires = qubits[0: len(qubits)])\n",
    "\n",
    "\t\t\t#now add the two-qubit circuit (F_2)\n",
    "\t\t\tparam0 = param1\n",
    "\t\t\tparam1 += U_params\n",
    "\t\t\tfor i in range(0, len(qubits), 2):\n",
    "\t\t\t\tU_SU4(params[param0: param1], wires = [qubits[i % len(qubits)], qubits[(i + 1) % len(qubits)]])\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(1, len(qubits), 2):\n",
    "\t\t\t\tU_SU4(params[param0: param1], wires = [qubits[i % len(qubits)], qubits[(i + 1) % len(qubits)]])\n",
    "\n",
    "\t\t\tparam0 = param1\n",
    "\t\t\tparam1 += len(qubits) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create the structure of the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quantum Circuits for Pooling layers\n",
    "param V: unitary which implements the pooling operation\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "\"\"\"\n",
    "def pooling_layer1(V, params):\n",
    "\tV(params, wires=[7, 6]) \n",
    "\tV(params, wires=[1, 0]) \n",
    "\n",
    "def pooling_layer2(V, params):\n",
    "\tV(params, wires=[3, 2]) \n",
    "\tV(params, wires=[5, 4]) \n",
    "\n",
    "def pooling_layer3(V, params, num_classes):\n",
    "\tif num_classes == 4: #if we need only 4 classes. we trace out another qubit\n",
    "\t\tV(params, wires=[2,0])\t\t\t\t   \n",
    "\tV(params, wires=[6,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the structure of the QCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It implements the structure of the QCNN\n",
    "param U: unitary F_1\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param num_classes: how many classes the QNN has to predict\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "\"\"\"\n",
    "def QCNN_structure(U, params, U_params, num_classes, num_layer):\n",
    "\t#divide the number of parameters for each layer: conv layer1, pooling layer 1, conv layer 2, ...\n",
    "\t#U_params indicates the number of parameters of the F_2 circuit (the circuit applied to couple of adjacent qubit)\n",
    "\t#n_qubit * 2: is the number of parameters for the circuit F_1\n",
    "\tparam1CL = params[0: (U_params + n_qubit * 2) * num_layer]\n",
    "\tparam1PL = params[(U_params + n_qubit * 2) * num_layer: ((U_params + n_qubit * 2) * num_layer) + 2]\n",
    "\t\t\n",
    "\tparam2CL = params[((U_params + n_qubit * 2) * num_layer) + 2: ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer)]\n",
    "\tparam2PL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer): \n",
    "\t\t\t\t\t  ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2]\n",
    "\n",
    "\tparam3CL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2: \n",
    "\t\t\t\t\t  ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer)]\n",
    "\n",
    "\t#apply the circuits\n",
    "\tconv_layer(U, param1CL, U_params, num_layer, range(n_qubit))\n",
    "\tpooling_layer1(Pooling_ansatz, param1PL)\n",
    "\t\n",
    "\tconv_layer(U, param2CL, U_params, num_layer, [0, 2, 3, 4, 5, 6])\n",
    "\tpooling_layer2(Pooling_ansatz, param2PL)\n",
    "\t\n",
    "\tconv_layer(U, param3CL, U_params, num_layer, [0, 2, 4, 6])\n",
    "\n",
    "\t#if we have only 4, 6 or 8 classes, then we need another pooling layer and we need to trace out:\n",
    "\t#another qubit if we have 6 or 8 classes, because we need only 3 qubits to represent 6 or 8 classes\n",
    "\t#2 qubits if we have 4 classes, because we need only 2 qubits to represent 4 classes/states\n",
    "\t#if we have 10 classes, then we don't apply another pooling layer, because we need 4 qubits\n",
    "\tif num_classes == 4 or num_classes == 6 or num_classes == 8:\n",
    "\t\tparam3PL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer):\n",
    "\t\t\t\t\t\t ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer) + 2]\t  \n",
    "\n",
    "\t\tpooling_layer3(Pooling_ansatz, param3PL, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the QCNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the simulator and the QCNN: encoding + VQC + measurement\n",
    "param X: sample in input\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: which encoding is chosen\n",
    "param num_classes: how many classes the QNN has to predict\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "return result: the probabilities of the states, which are associated to the MNIST classes\n",
    "\"\"\"\n",
    "dev = qml.device('default.qubit', wires = n_qubit)\n",
    "@qml.qnode(dev)\n",
    "def QCNN(X, params, U_params, embedding_type='amplitude', num_classes=10, num_layer = 1):\n",
    "\t# Data Embedding\n",
    "\tif embedding_type == 'amplitude':\n",
    "\t\tAmplitudeEmbedding(X, wires=range(8), normalize=True)\n",
    "\telif embedding_type == 'angle':\n",
    "\t\tAngleEmbedding(X, wires=range(8), rotation='Y')\n",
    "\t\n",
    "\t# Create the VQC\n",
    "\tQCNN_structure(CC14, params, U_params, num_classes, num_layer)\n",
    "\t\n",
    "\t#Measures the necessary qubits\n",
    "\tif num_classes == 4:\n",
    "\t\tresult = qml.probs(wires=[0, 4])\n",
    "\telif num_classes == 6:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4])\n",
    "\telif num_classes == 8:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4])\n",
    "\telse:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4, 6])\t\t\t\n",
    "\t\t\t\t\t\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It computes the cross-entropy loss\n",
    "param labels: correct classes of the Training set\n",
    "param predictions: classes predicted by the QCNN\n",
    "param num_classes: number of classes\n",
    "return loss: average loss \n",
    "\"\"\"\n",
    "def cross_entropy(labels, predictions, num_classes):\n",
    "\tepsilon = 1e-15\n",
    "\tnum_samples = len(labels)\n",
    "\n",
    "\tnum_classes = len(predictions[0])\n",
    "\tY_true_one_hot = anp.eye(num_classes)[labels]\n",
    "\n",
    "\tloss = 0.0\n",
    "\tfor i in range(num_samples):\n",
    "\t\tpredictions[i] = anp.clip(predictions[i], epsilon, 1 - epsilon)\n",
    "\t\tloss -= anp.sum(Y_true_one_hot[i] * anp.log(predictions[i]))\t\t\n",
    "\t\n",
    "\t\n",
    "\treturn loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It executes the circuit for each image of the dataset (divided in batches)\n",
    "param calculate the loss function\n",
    "param params: the angle to be trained\n",
    "param X: batches of the training set\n",
    "param Y: batches of the training set\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: indicate the chosen encoding )\n",
    "param circ_layer: number of repetitions of the convolutional layer\n",
    "return loss: average loss\n",
    "\"\"\"\n",
    "def cost(params, X, Y, U_params, embedding_type, num_classes, circ_layer):\n",
    "\tpredictions = [QCNN(x, params, U_params, embedding_type, num_classes=num_classes, num_layer = circ_layer) for x in X]\n",
    "\t\n",
    "\t\n",
    "\tloss = cross_entropy(Y, predictions, num_classes)\n",
    "\t\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss History for circuit with amplitude\n",
      "EPOCH:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zd/Library/Python/3.9/lib/python/site-packages/autograd/numpy/numpy_vjps.py:698: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  onp.add.at(A, idx, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  cost:  2.856018650950784\n",
      "EPOCH:  1\n",
      "iteration:  0  cost:  2.500451988125301\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It executes the training of the QNN\n",
    "param X_train: X training set\n",
    "param Y_train: Y training set\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: the encoding method used\n",
    "param num_classes: number of classes\n",
    "param num_layer: number of repetitions of conv layer\n",
    "param loadParams: if True the parameters are loaded from a file (used to continue a stopped training)\n",
    "param optimizer: the optimizer used\n",
    "param learning_rate: learning rate of the optimizer\n",
    "param epochs: number of epochs\n",
    "param all_samples: it all the samples are used\n",
    "param batch_size: size of the batches\n",
    "param seed: if None a random seed is used, otherwise the value in the variable\n",
    "return params: the trained parameters\n",
    "\"\"\"\n",
    "def circuit_training(X_train, Y_train, U_params, embedding_type, num_classes, num_layer, loadParams, optimizer, learning_rate, epochs, all_samples, batch_size, seed):\n",
    "\tif seed != None:\n",
    "\t\tnp.random.seed(seed)\n",
    "\t\tanp.random.seed(seed)\n",
    "\t\n",
    "\t#calculate the number of parameters\n",
    "\tif num_classes == 10:\n",
    "\t\ttotal_params =\t((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer)\n",
    "\telse: #we have to add another pooling layer at the end, so we need two parameters\n",
    "\t\ttotal_params =\t((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer) + 2\n",
    "\t\n",
    "\t#laod the parameters\n",
    "\tif not loadParams:\n",
    "\t\tparams = np.random.randn(total_params, requires_grad=True)\n",
    "\telse:\n",
    "\t\tfileParams = open('params' + 'L' + str(num_layer) + 'LR' + str(learning_rate) + optimizer + 'C' + str(num_classes) + str(all_samples) + '.obj', 'rb')\n",
    "\n",
    "\t\tparams = pickle.load(fileParams)\n",
    "\t\tfileParams.close()\n",
    "\t\tprint(params)\n",
    "\t\n",
    "\t#choose the optimizer\n",
    "\tif optimizer == 'Adam':\n",
    "\t\topt = qml.AdamOptimizer(stepsize=learning_rate)\n",
    "\telif optimizer == 'GDO':\n",
    "\t\topt = qml.GradientDescentOptimizer(stepsize=learning_rate)\n",
    "\telse:\n",
    "\t\topt = qml.QNGOptimizer(stepsize=learning_rate)\n",
    "\t\n",
    "\t#start the training\n",
    "\tloss_history = []\n",
    "\tgrad_vals = []\n",
    "\tfor e in range(0, epochs):\n",
    "\t\tprint(\"EPOCH: \", e)\n",
    "\t\tfor b in range(0, len(X_train), batch_size):\n",
    "\t\t\tif (b + batch_size) <= len(X_train):\n",
    "\t\t\t\tX_batch = [X_train[i] for i in range(b, b + batch_size)]\n",
    "\t\t\t\tY_batch = [Y_train[i] for i in range(b, b + batch_size)]\n",
    "\t\t\telse:\n",
    "\t\t\t\tX_batch = [X_train[i] for i in range(b, len(X_train))]\n",
    "\t\t\t\tY_batch = [Y_train[i] for i in range(b, len(X_train))]\n",
    "\n",
    "\t\t\tif optimizer == 'QNGO': \n",
    "\t\t\t\tmetric_fn = lambda p: qml.metric_tensor(QCNN, approx=\"block-diag\")(X_batch, p, U_params, embedding_type, num_classes, num_layer)\n",
    "\t\t\t\tparams, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U_params, embedding_type, num_classes, num_layer),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t \tparams, metric_tensor_fn=metric_fn)\n",
    "\t\t\telse:\n",
    "\t\t\t\tparams, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U_params, embedding_type, num_classes, num_layer),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t \tparams)\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif b % (batch_size * 100) == 0:\n",
    "\t\t\t\tprint(\"iteration: \", b, \" cost: \", cost_new)\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tloss_history.append(cost_new)\n",
    "\t\t\t\tgradient_fn = qml.grad(cost)\n",
    "\t\t\t\tgradients = gradient_fn(params, X_batch, Y_batch, U, U_params, embedding_type, cost_fn, num_classes, num_layer)\n",
    "\t\t\t\tgrad_vals.append(gradients[-1])\n",
    "\t\t\t\tprint(gradients)\n",
    "\t\t\t\tprint(\"var \", np.var(grad_vals))\n",
    "\t\t\t\tprint(\"mean grad: \", np.mean(grad_vals))\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t#save the novel parameters at the end of each epoch\t\n",
    "\t\tfileParams = open('params' + 'L' + str(num_layer) + 'LR' + str(learning_rate) + optimizer + 'C' + str(num_classes) + str(all_samples) + '.obj', 'wb')\n",
    "\n",
    "\n",
    "\t\tpickle.dump(params, fileParams)\n",
    "\t\tfileParams.close()\n",
    "\treturn params\n",
    "\n",
    "print(\"Loss History for circuit with \" + encoding)\n",
    "trained_params = circuit_training(X_train, Y_train, U_params, encoding, num_classes, num_layer, load_params,\n",
    "\t\t\t\t\t\t\t\topt, lr, epochs, all_samples, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3789\n",
      "[[[7845 1175]\n",
      "  [ 261  719]]\n",
      "\n",
      " [[7628 1237]\n",
      "  [  75 1060]]\n",
      "\n",
      " [[8388  580]\n",
      "  [ 839  193]]\n",
      "\n",
      " [[8215  775]\n",
      "  [ 651  359]]\n",
      "\n",
      " [[8900  118]\n",
      "  [ 913   69]]\n",
      "\n",
      " [[8774  334]\n",
      "  [ 765  127]]\n",
      "\n",
      " [[8814  228]\n",
      "  [ 563  395]]\n",
      "\n",
      " [[8514  458]\n",
      "  [ 637  391]]\n",
      "\n",
      " [[8424  602]\n",
      "  [ 692  282]]\n",
      "\n",
      " [[8287  704]\n",
      "  [ 815  194]]]\n",
      "precision 0: 0.3796198521647307\n",
      "recall 0: 0.7336734693877551\n",
      "f1 0: 0.5003479471120389\n",
      "precision 1: 0.46147148454505876\n",
      "recall 1: 0.933920704845815\n",
      "f1 1: 0.6177156177156177\n",
      "precision 2: 0.24967658473479948\n",
      "recall 2: 0.187015503875969\n",
      "f1 2: 0.21385041551246528\n",
      "precision 3: 0.3165784832451499\n",
      "recall 3: 0.35544554455445543\n",
      "f1 3: 0.33488805970149244\n",
      "precision 4: 0.3689839572192513\n",
      "recall 4: 0.07026476578411406\n",
      "f1 4: 0.11804961505560302\n",
      "precision 5: 0.2754880694143167\n",
      "recall 5: 0.14237668161434977\n",
      "f1 5: 0.18773096821877297\n",
      "precision 6: 0.6340288924558587\n",
      "recall 6: 0.4123173277661795\n",
      "f1 6: 0.49968374446552805\n",
      "precision 7: 0.46054181389870436\n",
      "recall 7: 0.38035019455252916\n",
      "f1 7: 0.4166222695791155\n",
      "precision 8: 0.3190045248868778\n",
      "recall 8: 0.28952772073921973\n",
      "f1 8: 0.30355220667384275\n",
      "precision 9: 0.21603563474387527\n",
      "recall 9: 0.19226957383548068\n",
      "f1 9: 0.20346093340325108\n"
     ]
    }
   ],
   "source": [
    "\t\n",
    "\"\"\"\n",
    "It computes the accuracy on the test set\n",
    "param predictions: classes predicted\n",
    "param labels: true classes\n",
    "param num_classes: number of classes\n",
    "return accuracy: accuracy \n",
    "\"\"\"\n",
    "def accuracy_multi(predictions, labels, num_classes):\n",
    "\tcorrect_predictions = 0\n",
    "\n",
    "\t\n",
    "\tfor l, p in zip(labels, predictions):\n",
    "\t\tp2 = []\n",
    "\t\tfor i in range(0, num_classes):\n",
    "\t\t\tp2.append(p[i])\n",
    "\t\tpredicted_class = np.argmax(p2)\t# Find the index of the predicted class with highest probability\n",
    "\t\tif predicted_class == l:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\n",
    "\taccuracy = correct_predictions / len(labels)\n",
    "\treturn accuracy\n",
    "\n",
    "\"\"\"\n",
    "It computes the precision, recall, F1 score and Confusion Matrix on the test set\n",
    "param predictions: classes predicted\n",
    "param labels: true classes\n",
    "param num_classes: number of classes\n",
    "return accuracy: accuracy \n",
    "\"\"\"\n",
    "def accuracy_test_multiclass(predictions, label, num_classes):\n",
    "\t#confusion matrix\n",
    "\t\n",
    "\tpreds_np = np.array(predictions)\n",
    "\tpreds = np.argmax(preds_np[:, :num_classes], axis = 1)\n",
    "\t\n",
    "\tconf_mat = multilabel_confusion_matrix(label, preds, labels = list(range(num_classes)))\n",
    "\tprint(conf_mat)\n",
    "\tprecision = []\n",
    "\trecall = []\n",
    "\tf1 = []\n",
    "\ti = 0\n",
    "\tfor c in conf_mat:\n",
    "\t\tprecision.append(c[1][1] / (c[1][1] + c[0][1]))\n",
    "\t\trecall.append(c[1][1] / (c[1][1] + c[1][0]))\n",
    "\t\tf1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + np.finfo(float).eps))\n",
    "\t\t\n",
    "\t\tprint(\"precision \" + str(i) + \": \" + str(precision[i])) \n",
    "\t\tprint(\"recall \" + str(i) + \": \" + str(recall[i])) \n",
    "\t\tprint(\"f1 \" + str(i) + \": \" + str(f1[i])) \n",
    "\t\ti += 1\n",
    "\n",
    "predictions = []\n",
    "\t\t\t\t\n",
    "for x in X_test:\t\n",
    "\tpredictions.append(QCNN(x, trained_params, U_params, encoding, num_classes, num_layer))\n",
    "\t\t\t\t\t\t\t\n",
    "accuracy = accuracy_multi(predictions, Y_test, num_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "accuracy_test_multiclass(predictions, Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
